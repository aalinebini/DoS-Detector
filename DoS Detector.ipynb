{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import socket as sk\n",
    "import struct as st\n",
    "import datetime as dt\n",
    "import ipaddress as ip\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the bad guys, dah!\n",
    "\n",
    "# slowloris \n",
    "slowloris_low  = st.unpack('!I', sk.inet_aton('10.128.0.1'))[0]\n",
    "slowloris_high = st.unpack('!I', sk.inet_aton('10.128.0.50'))[0]\n",
    "\n",
    "# slowhttptest\n",
    "slowhttptest_low  = st.unpack('!I', sk.inet_aton('10.128.0.50'))[0]\n",
    "slowhttptest_high = st.unpack('!I', sk.inet_aton('10.128.0.100'))[0]\n",
    "\n",
    "# slowloris_ng\n",
    "slowloris_ng_low  = st.unpack('!I', sk.inet_aton('10.128.0.100'))[0]\n",
    "slowloris_ng_high = st.unpack('!I', sk.inet_aton('10.128.0.150'))[0]\n",
    "\n",
    "# defining the TCP flags\n",
    "tcp_flags = [2, 4, 16, 17, 18, 20, 24, 25, 82, 144, 152, 194]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prequel Preprocessing\n",
    "* this function aims to cast the raw data into a 10 base integer represetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prequelProcessing(dataset):\n",
    "    \n",
    "    # Casting IP to a single integer\n",
    "    dataset['source_ip'] = dataset.source_ip.apply(lambda x: st.unpack('!I', sk.inet_aton(x))[0])\n",
    "    dataset['dest_ip'] = dataset.dest_ip.apply(lambda x: st.unpack('!I', sk.inet_aton(x))[0])\n",
    "    \n",
    "    # Casting Hexa to decimal base\n",
    "    dataset['tcp_flag'] = dataset.tcp_flag.apply(lambda x: int(x, 16))\n",
    "    \n",
    "    # Parsing string to datetime object\n",
    "    dataset['date'] = dataset['date'] + ' ' + dataset['time']\n",
    "    dataset['date'] = pd.to_datetime(dataset['date'], format='%Y%m%d %H:%M:%S', utc=True)\n",
    "    \n",
    "    # Getting rid of useless columns\n",
    "    dataset.drop(columns=['data', 'time'], inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(grouped_data): \n",
    "    \n",
    "    number_requisitions = np.sum(grouped_data['dest_port'] == 80) + np.sum(grouped_data['dest_port'] == 443)\n",
    "    number_different_destinations = len(np.unique(grouped_data['dest_ip']))\n",
    "    mean_frame_length = grouped_data['frame_length'].mean()\n",
    "    \n",
    "    data = {\n",
    "            'number_requisitions'           : [number_requisitions], \n",
    "            'number_different_destinations' : [number_different_destinations], \n",
    "            'mean_frame_length'             : [mean_frame_length]\n",
    "           }\n",
    "\n",
    "    for flag in tcp_flags:\n",
    "        data['flag_' + str(flag)] = [np.sum(grouped_data['tcp_flag'] == flag)]\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turnToPercentil(dataset, summary, column_name):\n",
    "\n",
    "    for i in range(len(summary.index)):\n",
    "        \n",
    "        if summary[column_name][i] > 0:\n",
    "\n",
    "            data_percentil = dataset.loc[summary.index[i], column_name] / summary[column_name][i]\n",
    "            dataset.loc[summary.index[i], column_name] = data_percentil.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizationPerTimePeriod(dataset):\n",
    "    \n",
    "    summary = dataset.groupby('date').sum()\n",
    "    \n",
    "    column_names= dataset.columns.values\n",
    "    column_names = np.delete(column_names, 2)\n",
    "    \n",
    "    for column in column_names:\n",
    "        \n",
    "        turnToPercentil(dataset, summary, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLabelColumn(grouped):\n",
    "    \n",
    "    # setting all IPs with none intruser type\n",
    "    grouped['y'] = 0\n",
    "    \n",
    "    # resetting the index \n",
    "    dataset = grouped.reset_index()\n",
    "    \n",
    "    # finding the bad guys\n",
    "    slowloris    = (dataset.source_ip >= slowloris_low) & (dataset.source_ip < slowloris_high)\n",
    "    slowhttptest = (dataset.source_ip >= slowhttptest_low) & (dataset.source_ip < slowhttptest_high)\n",
    "    slowloris_ng = (dataset.source_ip >= slowloris_ng_low) & (dataset.source_ip < slowloris_ng_high)\n",
    "\n",
    "    # and labeling them\n",
    "    dataset.loc[slowloris, 'y']    = 1\n",
    "    dataset.loc[slowhttptest, 'y'] = 2\n",
    "    dataset.loc[slowloris_ng, 'y'] = 3\n",
    "    \n",
    "    # resuming the original index\n",
    "    dataset.set_index(['date', 'source_ip'], inplace=True)\n",
    "    \n",
    "    # getting rid of useless columns\n",
    "    dataset.drop(columns=['level_2'], inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(packages, frequency):\n",
    "    \n",
    "    dataset = prequelProcessing(packages)\n",
    "    \n",
    "    grouped = dataset.groupby([\n",
    "            # groupping the data per a specific time frequency\n",
    "            pd.Grouper(key='date', freq=frequency), \n",
    "            # groupping the remaining data by the IPs\n",
    "            pd.Grouper(key='source_ip')\n",
    "            # Applying the function who will create the news features\n",
    "            ]).apply(features)\n",
    "    \n",
    "    # normalizing the data\n",
    "    normalizationPerTimePeriod(grouped)\n",
    "    \n",
    "    # generating the true label array\n",
    "    dataset = generateLabelColumn(grouped)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running all over together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = list()\n",
    "\n",
    "total_lines = !wc -l Complements/TCP1.csv | grep -Eo '^[0-9]+'\n",
    "total_lines = int(total_lines[0]) \n",
    "number_of_lines = 10000\n",
    "\n",
    "total_start = perf_counter()\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv('Complements/TCP1.csv', \n",
    "                                   low_memory=False, \n",
    "                                   index_col=[0], \n",
    "                                   chunksize=number_of_lines)):\n",
    "\n",
    "    porcentage = (i * number_of_lines) / total_lines\n",
    "    \n",
    "    print(round(porcentage * 100, 2), '% complete')\n",
    "    \n",
    "    preprocessed_chunk = preprocessing(chunk, '10s')\n",
    "    collection.append(preprocessed_chunk)\n",
    "    \n",
    "total_stop = perf_counter()\n",
    "\n",
    "print('#'*50, '\\nTotal time:', total_stop - total_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "packages = pd.read_csv('Complements/TCP1.csv', \n",
    "                                   low_memory=False, \n",
    "                                   index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.preprocessing import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying prequel processing\n",
      "Generating the features\n",
      "Normalizing the features\n",
      "Creating the true label array\n"
     ]
    }
   ],
   "source": [
    "dataset = preprocessing(packages, '20s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
